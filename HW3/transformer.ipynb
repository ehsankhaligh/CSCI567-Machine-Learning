{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cPGBtkkP3WDS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113b0b190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-xXM4CrvAXc"
   },
   "source": [
    "# Let's build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ibQjrqnvKby"
   },
   "source": [
    "We will first implement self-Attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "A-ab2mTkvJhK"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "\n",
    "          (1) Computes key and query representations using linear transformations.\n",
    "          (2) Computes attention scores by multiplying query and key tensors and normalizing by C**-0.5.\n",
    "              Masks out future information using the lower triangular matrix (you can use tril function).\n",
    "              Applies softmax to get attention weights. Then, applies dropout for regularization\n",
    "          (3) Computes the weighted sum of values using attention weights.\n",
    "        \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "\n",
    "          (1) Computes attention for each head in parallel and concatenates their outputs along the last dimension.\n",
    "          (2) Projects concatenated head outputs back to the original dimension using a linear layer.\n",
    "          (3) Applies dropout for regularization.\n",
    "        \"\"\"\n",
    "\n",
    "        ##################################################################\n",
    "        #\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        #\n",
    "        ##################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmr-yFRO0YEU"
   },
   "source": [
    "It's turn for FeedForward module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "keRYFV380XL-"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        ##################################################################\n",
    "        #\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        #\n",
    "        ##################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "        \"\"\"\n",
    "\n",
    "        ##################################################################\n",
    "        #\n",
    "        return self.net(x)\n",
    "        #\n",
    "        ##################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgHsspJ809d_"
   },
   "source": [
    "We have all elements for creating a block now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8bsewCbA0816"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "          (1) Utilizes layer normalization to preprocess the input features before passing them through the self-attention mechanism.\n",
    "              Updates the input features by adding the output of the self-attention mechanism, fostering communication between different parts of the input sequence.\n",
    "          (2) Utilizes layer normalization to preprocess the updated input features before passing them through the feedforward neural network.\n",
    "              Updates the input features again by adding the output of the feedforward neural network, enabling the model to capture non-linear relationships and patterns within the data.\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G91V9Ora11xp"
   },
   "source": [
    "We can now create our toy model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "P-s2Z6yn2EcK"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        ##################################################################\n",
    "        #\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        #\n",
    "        ##################################################################\n",
    "\n",
    "        #compute loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_fC913PrD7Q"
   },
   "source": [
    "# Now, it's time to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY0ZIAnXGx3l"
   },
   "source": [
    "First, download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Dp5wTKOsrX0u"
   },
   "outputs": [],
   "source": [
    "#Download tiny Shakespeare dataset and read it\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frIeUGEoraWG"
   },
   "source": [
    "Now, we will simply implement our own tokenizer. For this simple assignment, we will use __character based tokenizer__. However, keep in mind that character level tokenizer is not a common practice in real life. More complicated tokenizers are being used. For an introductory reading to tokenizers, you refer to the following video: https://youtu.be/zduSFxRajkE?si=quNy7UNMQkJ20Nim\n",
    "\n",
    "There are __two main properties of a tokenizer: encode and decode__. Encoding function takes a string, divides it to the tokens and then converts each token to their assigned integer value. You can think about this integer assignment part as a lookup table. Decoding reverses the encoding process, converting the processed or encoded text back into its original form, allowing for human-readable interpretation or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z9QNGk__rQHK"
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haWjuqkDtvpz"
   },
   "source": [
    "Let's create train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NzAGM5oKt09S"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mEsE9xLQuBNh"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XpAElNyluf6l"
   },
   "outputs": [],
   "source": [
    "#define loss estimation function for train and validation\n",
    "@torch.no_grad()\n",
    "def estimate_loss(batch_size, block_size, eval_iters = 200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrNQ8Fu6G4Bp"
   },
   "source": [
    "Define hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tfV-teYy2STp"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 64\n",
    "#n_head = 4\n",
    "n_head = 2\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IpYQE174mkf"
   },
   "source": [
    "Create model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMybKo9-4bnN",
    "outputId": "c4842122-2456-4982-d089-119ff7815723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvGzmMqV8u02"
   },
   "source": [
    "Let's see what model generates before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TTWAw5e08tLo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DuJAMJ3fX$VWoGCYngj$'onoE,$AT-NvynticCzJ-c!bnDNyjfND,?to;rVcANzq:nf$3NVAz:YzgNkceei.-efVhpXz,NApQe.3hRfNThglUzoecXgZ q.g?d\n",
      "xeo$ug:'QmZqR3F!neLZmh.TW,$-Jko.iu!seNwol$zJ,pJhwiKeo,Ssnu3blKpSpbMcSWhWmYn&UwNUc-qmfjkeRecJDeqqNWVa-?gLrR XbM ?BJxRlB;?zUC\n",
      "blOHdJV\n",
      "XXo;Vch.kt?Hpza:hoV.NarnDozmp zPj-!gTsoSN-FgAAuYeANEmUhNMNx- SEMxyJdyJAssmp.! OKoc,riiJ?acNQKscoiiNzAp,zciDvy-uFXQV'Vquq$Bvd-OuH,$AyEqs.,wUcLOa e$':,cJwX'TOiaBpyokP&W!T!NGUUJWS.OWo!LvFjeDfP.?,FoJgu!cXuImjNE;,jqthn,y?3T:EiOBpLm\n",
      ":OgMb,xE,ol uHcI KZJKFcZpWDn3,XY\n",
      "JVP'KNvdrhk!pJQJnsW3pvJ-b3,QPD3NN DfVPUeo.HwouBh\n",
      "vr:wPpu!iuNJXENzHVNFq,PzeK?h.INNcmF,k,oc$qU$iuJCzuSbA$Wn,dpoPAeOzdX,nBoOzQ!Ldnsi?sAriBO.bqjouA :$VOi.&AlANczQ,yb jv.-qubZo!RN-QtAL;V:rQ,klRMsVhdypppLyGKeE-IoowJpXKMTwNxLegKUjpEzq! dZpupUetU\n",
      "q D!Dl--q\n",
      ",aM?W-cAL$gli;H;iivwo,RoQiaaL.C-T.&p&\n",
      "\n",
      "p?AJHyp'OCplq3tU!IEcpodmKKPqrHe$C.,g:-A'ONJep,XignPXoOn!qUDFC;URWzXUzELD3D3N&CEOWUwXcJp,JlKsp, Dftwp:NGsIR.?cetPme?p3f'-g?,BBX,ciFzeXDt&V!rnoOwtBp\n",
      "hioZNLLeyhL!sV\n",
      "QvqJJlbup?UJWbJ?Upei&,\n",
      "LeTanqJqJDzmp!wj&c.cYNwUOAKAf-sHpsQeJMN3cMtZLNc!$XhSKN\n",
      "V;EmUcNf-jaLwphC-aVD zm\n",
      "putgO,;k$?'jn$-Mpvc'aehlKU& LdK'CJyNPztVt-,hgeCqUWh,DXgeosNjedVtQDug&pazXQcaYMmd.eHEsDwNcPN:\n",
      ";3\n",
      "V.uF$pbqppKhFNzbThguFw3AQJn?yo!$en?hyV;sWzdxpjtFheuczN Tio\n",
      "gNP:QcAcPomMucbvioWTzNkkZNpVvEtyzlKOpZNx\n",
      "iI$znI3QK,.XJrXtcI alc!JKWNYGxTwGcdsk&pOGfpsM:Q?KhDHcbYnczfdhplPvjKUEp$Qtvkwhvtnl$co?wU-KnDv-?itpKFD'ob czqLMSgkzOYIcArdpk;3io&AzCYEl,ViMUpFcLU-nOQucpxt-.KCe3VhkCPfVFAiO&OzuNc$3.AftKczSGi J--DacfLde,e$,3?$otzXVwV:zmiof?.VuBjewEgn$JQ.UfodpmsgNHpVcJwjLQ,FRzwbYvF,DoFoQTeCE.AKcJiXkauNPi?\n",
      "&dmquhv'Ob?n-Pq:$ApDKtA,iW-KcuKw?.;FKdX;V kf?!yNjyWhoe!zcM:Az'n,KzyoEh-cu;AVp:tj\n",
      "FpU.-?a:bd3L:,fglJeylFowtcNnUpGoLQqL3spLUr:-JKp,Z :JcJwzqtsppIueci:twBuNMpbtl:!IcNz'FsqzpMutpqo'.-$3DXVOsVnctgVpq$AJRS.-gwn&yFuNwqwnQcLaVFcSvXizCr;p;zrqrnpKRSjaV.fq3mecJkte?hfgwfdheLMwz!,PBa\n",
      "JCJ?$-GKrTy?-m-HG:znWEgj!T.pD3fMBqu&ywKPT$&?RgmnoE-NrpONKkSNKdOoXPHPSfbBXuppDwXsleVctcL,G,pXpcsgzya:edz-XP&pHQuXlWTUAA-lphf'Z! pwFgcWKnWmMWXKgxcZXBgUvSup&XRgmrDmONcPnNjBNcX-FKiXNe'\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMjWNQ3p4sFM"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hoelkOrFY8bN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4045, val loss 4.3964\n",
      "step 100: train loss 2.6368, val loss 2.6426\n",
      "step 200: train loss 2.4858, val loss 2.4908\n",
      "step 300: train loss 2.3788, val loss 2.3852\n",
      "step 400: train loss 2.3180, val loss 2.3250\n",
      "step 500: train loss 2.2627, val loss 2.2680\n",
      "step 600: train loss 2.2226, val loss 2.2254\n",
      "step 700: train loss 2.1587, val loss 2.1800\n",
      "step 800: train loss 2.1203, val loss 2.1546\n",
      "step 900: train loss 2.0891, val loss 2.1332\n",
      "step 1000: train loss 2.0662, val loss 2.1016\n",
      "step 1100: train loss 2.0332, val loss 2.0733\n",
      "step 1200: train loss 2.0073, val loss 2.0498\n",
      "step 1300: train loss 1.9925, val loss 2.0349\n",
      "step 1400: train loss 1.9589, val loss 2.0242\n",
      "step 1500: train loss 1.9411, val loss 2.0177\n",
      "step 1600: train loss 1.9129, val loss 1.9930\n",
      "step 1700: train loss 1.9113, val loss 2.0010\n",
      "step 1800: train loss 1.8943, val loss 1.9807\n",
      "step 1900: train loss 1.8736, val loss 1.9770\n",
      "step 2000: train loss 1.8561, val loss 1.9568\n",
      "step 2100: train loss 1.8533, val loss 1.9759\n",
      "step 2200: train loss 1.8266, val loss 1.9369\n",
      "step 2300: train loss 1.8221, val loss 1.9386\n",
      "step 2400: train loss 1.8014, val loss 1.9408\n",
      "step 2500: train loss 1.7996, val loss 1.9428\n",
      "step 2600: train loss 1.7911, val loss 1.9320\n",
      "step 2700: train loss 1.7785, val loss 1.9193\n",
      "step 2800: train loss 1.7757, val loss 1.9091\n",
      "step 2900: train loss 1.7598, val loss 1.8999\n",
      "step 3000: train loss 1.7468, val loss 1.8887\n",
      "step 3100: train loss 1.7423, val loss 1.8754\n",
      "step 3200: train loss 1.7253, val loss 1.8651\n",
      "step 3300: train loss 1.7205, val loss 1.8617\n",
      "step 3400: train loss 1.7183, val loss 1.8645\n",
      "step 3500: train loss 1.7056, val loss 1.8696\n",
      "step 3600: train loss 1.7177, val loss 1.8592\n",
      "step 3700: train loss 1.7095, val loss 1.8415\n",
      "step 3800: train loss 1.6991, val loss 1.8679\n",
      "step 3900: train loss 1.6917, val loss 1.8531\n",
      "step 4000: train loss 1.6940, val loss 1.8513\n",
      "step 4100: train loss 1.6868, val loss 1.8377\n",
      "step 4200: train loss 1.6771, val loss 1.8434\n",
      "step 4300: train loss 1.6878, val loss 1.8547\n",
      "step 4400: train loss 1.6646, val loss 1.8507\n",
      "step 4500: train loss 1.6734, val loss 1.8551\n",
      "step 4600: train loss 1.6719, val loss 1.8390\n",
      "step 4700: train loss 1.6589, val loss 1.8200\n",
      "step 4800: train loss 1.6537, val loss 1.8061\n",
      "step 4900: train loss 1.6513, val loss 1.7983\n",
      "step 4999: train loss 1.6386, val loss 1.8149\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(batch_size, block_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        train_loss.append(losses['train'])\n",
    "        val_loss.append(losses['val'])\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "plt.plot(train_loss, label=\"train\")\n",
    "plt.plot(val_loss, label=\"val\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-kA8bXe82IK"
   },
   "source": [
    "Now, it's time to see the generation of a trained model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fireaticent\n",
      "Awhat, lame as a hold swo helper\n",
      "To soul depine is untagent one to gliff thy battless were the hears come,\n",
      "Nor Oxcessly: sir, by husil.\n",
      "\n",
      "LADY IUS:\n",
      "Let eyes guily my eyes full.\n",
      "When that rest by rather's genemies.\n",
      "\n",
      "Fillo, Jureof\n",
      "Make wetchirt with here care this lause.\n",
      "In stay not him than souly distard grount a sween:\n",
      "Ah, what her man, but in noth hanged my bargued;\n",
      "And, therefor SmOUCESTER:\n",
      "My say and grace.\n",
      "3 HENRY Bold soveraly master you respirount off\n",
      "uperse; by confuccy: so with fhile his prity. No's but use this is thy pards it. But fath he, death of yetempt,\n",
      "And father not gasned towe is any sope is as was shall kin me;\n",
      "When in things himself shames, then less,\n",
      "A son prom mieed your last, deeppuled?\n",
      "\n",
      "JRITH:\n",
      "Wherew, and my this calens,\n",
      "With lew'd and done the when noped caugness.\n",
      "Bathen untient of ways? Why, so good,\n",
      "Mercupard, the glot man me wear the part.\n",
      "\n",
      "PORISSOLANUS:\n",
      "The mustman you good feap the world,\n",
      "From Leem Catter of iyour say, well:\n",
      "thou obst day dothing of despil Cichior:\n",
      "The you slain, beithes latked. That prities,\n",
      "Thinks thy glaty'd common my could:\n",
      "And kinj; leave as trmys have owards way,\n",
      "With my guictines to Rispeak us yield!\n",
      "\n",
      "DUKE ANVOLANNT:\n",
      "HOMBY the are is day fall are oxid.\n",
      "\n",
      "JULIA:\n",
      "I ambreach, as Kees: sparman.\n",
      "DOHESS kine, face imselver: thee how were night.\n",
      "\n",
      "CAMILLEO:\n",
      "Thought musn my pross again.\n",
      "\n",
      "PLARET:\n",
      "'Has as then sover it kin, as juster to much art thie deselittely nichion.\n",
      "\n",
      "Secots:\n",
      "Now, 'xerefore that we mlack that or therefor'dow. Whiss not.\n",
      "\n",
      "BlowNARWILLIFF:\n",
      "To Narfieuless if the shows hear\n",
      "The hast in Jareoper'd of youNGo you and both makes,\n",
      "With shonescarded no his shuntry shaper'd sover your dame!\n",
      "\n",
      "DUCH:\n",
      "Or this in our\n",
      "Too Keepour is the worksee mole.\n",
      "\n",
      "CLYCUS:\n",
      "YORAY:\n",
      "Nay, my unless gantle with thy time which. Jay, For first\n",
      "Thy peacefore to Seence she is must once,\n",
      "Cruch share your most's and seemser.\n",
      "3 PRICK:\n",
      "Yet for pler in sea\n",
      "Where not myself the troves; let's him, sins; if the larence the heart, that being? \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
